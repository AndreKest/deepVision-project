{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anmerkung: Im folgenden Skript gibt es keine Ausgaben, da ich das Modell mithilfe der im GPU-Labor zur Verfügung stehenden Rechner trainiert habe. Ich habe die unten aufgeführten Befehle in das Terminal eingegeben, dass ich es dort im Hintergrund trainieren lassen konnte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anmerkung Datensatz:\n",
    "Der Datensatz liegt in folgender Struktur vor:\n",
    "- annotations/\n",
    "    - instances_train.json   &emsp;&emsp; Informationen im COCO-Format über die Trainingsbilder\n",
    "    - instances_val.json  &emsp;&emsp; Informationen im COCO-Format über die Validationbilder\n",
    "- train2017/ Trainingsbilder\n",
    "- val2017/ Validationbilder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YoloX Training\n",
    "\n",
    "basiert auf dem YoloX GitHub Repository von Megvii-BaseDetection (https://github.com/Megvii-BaseDetection/YOLOX)\n",
    "\n",
    "**Ordnerstruktur:**\n",
    "\n",
    "yoloxModel  \n",
    "├── datasets                &emsp;&emsp; Datensatz im COCO-Format\n",
    "├── testImages              &emsp;&emsp; Testbilder für demo.py  \n",
    "├── weights                 &emsp;&emsp; Pre-Trained Gewichte aus dem Repository (Pre-Trained mit COCO-Datensatz)  \n",
    "├── yolox                   &emsp;&emsp; YoloX-Netzwerk  \n",
    "├── YOLOX_Outputs           &emsp; Ausgabe der Gewichte und Log-Dateien nach dem Training und der Vorhersage  \n",
    "├── requirements.txt        &emsp;&emsp; Requirements für die Installation der Pakete  \n",
    "├── train.py                &emsp;&emsp; Datei zum Trainieren  \n",
    "├── eval.py                 &emsp;&emsp; Datei zum Evaluieren  \n",
    "├── demo.py                 &emsp;&emsp; Datei zur Vorhersage  \n",
    "└── yolox_m.py              &emsp;&emsp; Konfigurationsdatei (hier für yolox in Größe m)  \n",
    "--> Andere Konfigurationsdateien für YoloX in einer anderen Größe können in dem GitHub Repository von Megvii-BaseDetection unter '/exps/default/' gefunden werden\n",
    "\n",
    "Die folgenden Skripte sind aus dem YoloX GitHub Repository von Megvii-BaseDetection\n",
    "- train.py Training des YoloX-Netzwerks\n",
    "- eval.py Validation des YoloX-Netzwerks\n",
    "- demo.py Vorhersage mit YoloX-Netzwerk\n",
    "- yolox_m.py Dort sind die Konfigurationen für das Yolo-Netzwerk in Größe Medium (m). Hyperparemeter werden dort konfiguriert und Pfade zum Datensatz angepasst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vor dem Training\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vor dem Training muss die requirements.txt installiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passe die yolox_m.py (Konfigurationsdatei) an:\n",
    "\n",
    "Bei dieser Datei handelt es sich um eine Klasse, die alle Hyperparameter beinhaltet, die für das YoloX-Netzwerk benötigt werden.\n",
    "Alle Default-Parameter können in folgedner Datei 'yolox/exp/yolox_base,p' eingesehen werden und müssen, falls nötig in der yolox_m.py überschrieben werden, da sonst diese Werte genommen werden.\n",
    "\n",
    "- self.depth = 0.67\n",
    "- self.width = 0.75\n",
    "- self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]  \n",
    "--> Parameter für Modell (ob nano, s, m, l, ...)  \n",
    "\n",
    "Meine angepassten Parameter:\n",
    "- self.data_dir = \"datasets/dataYoloX\"   &emsp;&emsp; # Ordner in dem die Datensatzordner annotation, train2017 und val2017 liegen\n",
    "- self.train_ann = \"instances_train.json\"  &emsp; # .json Datei mit den Informationen über die Trainingsdaten\n",
    "- self.val_ann = \"instances_val.json\" &emsp; # .json Datei mit den Informationen über die Validationdaten\n",
    "- self.num_classes = 5  &emsp;&emsp; # Anzahl der Klassen in dem Datensatz\n",
    "- self.max_epoch = 100  &emsp;&emsp; # Anzahl von Epochen\n",
    "- self.data_num_workers = 4  &emsp;&emsp; # Anzahl der Prozesse\n",
    "- self.eval_interval = 1  &emsp;&emsp; # Anzahl wie oft das netzwerk in dem Training evaluiert wird\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Beim Training werden die Gewichte der aktuellen Epoche, die besten Gewichte (best-ckpt.pth), das Trainings-Log (train_log.txt) und die Trainingsmetriken mithilfe von tensorboard gespeichert. Während des Trainings werden die Dateien unter dem Pfad 'YOLOX_outputs/yolox_m/' gespeichert.\n",
    "\n",
    "Aufgrund der Größe der Dateien sind hier nicht alle Gewichte abgespeichert, sondern nur die Letzten und die Besten."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für das Training mit Oversampling wurde der torch.utils.data.WeightedRandomSampler() von PyTorch genommen, dieser hat dann im code 'yolox/data/dataloading.py' den RandomSampler() ersetzt.<br>\n",
    "Die Ergebnisse mit den Default-Werten sind in 'YOLOX_outputs/yolox_m_100_epoch_no_Oversample'.<br>\n",
    "Die Ergebnisse mit den WeightedRandomSampler sind in 'YOLOX_outputs/yolox_m_100_epoch_Oversample'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter erklären:\n",
    "- -f Pfad zur Konfigurationsdatei\n",
    "- -d Anzahl der GPUs\n",
    "- -b Batchsize\n",
    "- -o Benutze GPU RAM\n",
    "- -c Pfad zu den  Pre-Trained Gewichten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zum ausführen des Trainings\n",
    "!python train.py -f yolox_m.py -d 1 -b 8 --fp16 -o -c weights/yolox_m.pth"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter erklären:\n",
    "- -n Yolo Modell (ob s: yolox-s oder m: yolox-m, ...)\n",
    "- -c Pfad zu den trainierten Modellgewichten\n",
    "- -d Anzahl der GPUs\n",
    "- -b Batchsize\n",
    "- --conf Threshold\n",
    "- -f Pfad zur Konfigurationsdatei\n",
    "- --test Verwendet testImages, sonst valImages\n",
    "\n",
    "Genaue Auswertung der Metriken steht in yoloxMetric.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 eval.py -n  yolox-m -c YOLOX_outputs/yolox_m_200_epoch/weights/best_ckpt.pth -b 8 -d 1 --conf 0.001 -f yolox_m.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Trifft eine Vorhersage auf ein angegebenes Bild. Das Ergebnis wird unter 'YOLOX_outputs/<yolox_model>/vis_res' abgespeichert, falls --save_result gesetzt ist."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter erklären:\n",
    "- -f Pfad zur Konfigurationsdatei\n",
    "- -c Pfad zu den Gewichten\n",
    "- --path Pfad zu dem Testfoto (.jpg, .png)\n",
    "- --conf Threshold\n",
    "- --nms Threshold (Non-max Suppresion)\n",
    "- --tsize Dimension des Testbildes (Bsp.: 512x512)\n",
    "- --save_result Das Ergebnis wird unter 'YOLOX_outputs/yolox_m/vis_res/' gespeichert\n",
    "- -- device Ob CPU oder GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-07-04 07:42:20.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m259\u001b[0m - \u001b[1mArgs: Namespace(demo='image', experiment_name='yolox_m', name=None, path='testImages/testImage_4.jpg', camid=0, save_result=True, exp_file='yolox_m.py', ckpt='YOLOX_outputs/yolox_m_200_epoch_no_Oversampling/weights/best_ckpt.pth', device='cpu', conf=0.25, nms=0.45, tsize=512, fp16=False, legacy=False, fuse=False, trt=False)\u001b[0m\n",
      "\u001b[32m2023-07-04 07:42:20.742\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m269\u001b[0m - \u001b[1mModel Summary: Params: 25.28M, Gflops: 47.19\u001b[0m\n",
      "\u001b[32m2023-07-04 07:42:20.743\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m282\u001b[0m - \u001b[1mloading checkpoint\u001b[0m\n",
      "\u001b[32m2023-07-04 07:42:20.933\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m286\u001b[0m - \u001b[1mloaded checkpoint done.\u001b[0m\n",
      "\u001b[32m2023-07-04 07:42:21.804\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minference\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mInfer time: 0.8590s\u001b[0m\n",
      "\u001b[32m2023-07-04 07:42:21.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mimage_demo\u001b[0m:\u001b[36m202\u001b[0m - \u001b[1mSaving detection result in ./YOLOX_outputs/yolox_m/vis_res/2023_07_04_07_42_20/testImage_4.jpg\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 demo.py image -f yolox_m.py -c YOLOX_outputs/yolox_m_200_epoch/weights/best_ckpt.pth --path testImages/testImage_1.jpg --conf 0.25 --nms 0.45 --tsize=512 --save_result --device gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
